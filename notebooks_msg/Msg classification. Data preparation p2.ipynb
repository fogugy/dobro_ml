{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost\n",
    "# !pip install gensim\n",
    "# !pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/tweet_cleaned.csv').sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108738</th>\n",
       "      <td>Азазаза ;3\\nМне начинает нравится это фото) ht...</td>\n",
       "      <td>0</td>\n",
       "      <td>азазаза начинает нравится это фото</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208661</th>\n",
       "      <td>Снова начинается подобная хрень:( http://t.co/...</td>\n",
       "      <td>1</td>\n",
       "      <td>снова начинается подобная хрень печаль</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52972</th>\n",
       "      <td>RT @VasilyevaDi: стоим дежурим:)вспомнили наши...</td>\n",
       "      <td>0</td>\n",
       "      <td>пользователь стоим дежурим вспомнили наших мал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59376</th>\n",
       "      <td>Ертенгі собрга 2 узын ниггер бірге барат) как ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ертенгі собрга узын ниггер бірге барат</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45607</th>\n",
       "      <td>Новоиспеченные -Александр Маршал и Эми Уайнхау...</td>\n",
       "      <td>0</td>\n",
       "      <td>новоиспеченные александр маршал эми уайнхауз р...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  target  \\\n",
       "108738  Азазаза ;3\\nМне начинает нравится это фото) ht...       0   \n",
       "208661  Снова начинается подобная хрень:( http://t.co/...       1   \n",
       "52972   RT @VasilyevaDi: стоим дежурим:)вспомнили наши...       0   \n",
       "59376   Ертенгі собрга 2 узын ниггер бірге барат) как ...       0   \n",
       "45607   Новоиспеченные -Александр Маршал и Эми Уайнхау...       0   \n",
       "\n",
       "                                              clean_tweet  \n",
       "108738                 азазаза начинает нравится это фото  \n",
       "208661             снова начинается подобная хрень печаль  \n",
       "52972   пользователь стоим дежурим вспомнили наших мал...  \n",
       "59376              ертенгі собрга узын ниггер бірге барат  \n",
       "45607   новоиспеченные александр маршал эми уайнхауз р...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-ruscorpora-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "pos_conv = {\n",
    "    \"A\": \"ADJ\",\n",
    "    \"ADV\": \"ADV\",\n",
    "    \"ADVPRO\": \"ADV\",\n",
    "    \"ANUM\": \"ADJ\",\n",
    "    \"APRO\": \"DET\",\n",
    "    \"COM\": \"ADJ\",\n",
    "    \"CONJ\": \"SCONJ\",\n",
    "    \"INTJ\": \"INTJ\",\n",
    "    \"NONLEX\": \"X\",\n",
    "    \"NUM\": \"NUM\",\n",
    "    \"PART\": \"PART\",\n",
    "    \"PR\": \"ADP\",\n",
    "    \"S\": \"NOUN\",\n",
    "    \"SPRO\": \"PRON\",\n",
    "    \"UNKN\": \"X\",\n",
    "    \"V\": \"VERB\",\n",
    "}\n",
    "\n",
    "default_word = 'человек_NOUN'\n",
    "\n",
    "def tag(word):\n",
    "    try:\n",
    "        processed = m.analyze(word)[0]\n",
    "        lemma = processed[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "        pos = processed[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "        pos = pos.split('=')[0].strip()\n",
    "        tagged = f\"{lemma}_{pos_conv.get(pos) or 'NOUN'}\"\n",
    "        return tagged\n",
    "    except:\n",
    "        return default_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(text, agg_func):\n",
    "    v = []\n",
    "    for w in text.split(' '):\n",
    "        tagged = tag(w)\n",
    "        v.append(model.get_vector(tagged) if model.vocab.get(tagged) else model.get_vector('человек_NOUN'))\n",
    "    return agg_func(np.array(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _avg(v):\n",
    "    return np.sum(v, axis=0)/v.shape[0]\n",
    "\n",
    "def _max(v):\n",
    "    return np.max(v, axis=0)\n",
    "\n",
    "def _min(v):\n",
    "    return np.min(v, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(default_word, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_v = model.get_vector(default_word)\n",
    "\n",
    "columns_feat = [f'min_{x}' for x in range(example_v.shape[0])] \\\n",
    "    + [f'max_{x}' for x in range(example_v.shape[0])] \\\n",
    "    + [f'avg_{x}' for x in range(example_v.shape[0])]\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(n_features=2**9)\n",
    "\n",
    "hash_cols = [f'hvec_{x}' for x in range(2**9)]\n",
    "\n",
    "df_vec = pd.DataFrame(columns=columns_feat+hash_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./msg_columns.pickle', 'wb') as f:\n",
    "    pickle.dump(columns_feat+hash_cols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./msg_hash_vec.pickle', 'wb') as f:\n",
    "    pickle.dump(hash_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat(text, aggr):\n",
    "    if type(text) is not str:\n",
    "        text = default_word\n",
    "    vec = get_vec(text, _min).tolist() + get_vec(text, _max).tolist() + get_vec(text, _avg).tolist()\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0:00:00\n",
      "1 0:00:12\n",
      "2 0:00:12\n",
      "3 0:00:11\n",
      "4 0:00:11\n",
      "5 0:00:11\n",
      "6 0:00:11\n",
      "7 0:00:12\n",
      "8 0:00:11\n",
      "9 0:00:11\n",
      "10 0:00:11\n",
      "11 0:00:12\n",
      "12 0:00:11\n",
      "13 0:00:11\n",
      "14 0:00:11\n",
      "15 0:00:11\n",
      "16 0:00:12\n",
      "17 0:00:11\n",
      "18 0:00:13\n",
      "19 0:00:11\n",
      "20 0:00:11\n",
      "21 0:00:11\n",
      "22 0:00:11\n",
      "23 0:00:12\n",
      "24 0:00:11\n",
      "25 0:00:12\n",
      "26 0:00:11\n",
      "27 0:00:12\n",
      "28 0:00:12\n",
      "29 0:00:11\n",
      "30 0:00:11\n",
      "31 0:00:12\n",
      "32 0:00:11\n",
      "33 0:00:12\n",
      "34 0:00:12\n",
      "35 0:00:11\n",
      "36 0:00:11\n",
      "37 0:00:12\n",
      "38 0:00:11\n",
      "39 0:00:12\n",
      "40 0:00:11\n",
      "41 0:00:12\n",
      "42 0:00:11\n",
      "43 0:00:11\n",
      "44 0:00:12\n",
      "45 0:00:12\n",
      "46 0:00:12\n",
      "47 0:00:11\n",
      "48 0:00:11\n",
      "49 0:00:12\n",
      "CPU times: user 6min 42s, sys: 52.9 s, total: 7min 35s\n",
      "Wall time: 9min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "h = np.array(hash_vectorizer.fit_transform(df['clean_tweet']).toarray())\n",
    "\n",
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "c = 1000\n",
    "\n",
    "for x in range(50):\n",
    "    df_vec_partial = pd.DataFrame(columns=columns_feat+hash_cols)    \n",
    "\n",
    "    i=0  \n",
    "\n",
    "    for index, row in df[x*c:(x+1)*c].iterrows():\n",
    "        if i % 1000 == 0:\n",
    "            b = datetime.datetime.now().replace(microsecond=0)\n",
    "            print(x, b-a)\n",
    "            a = b\n",
    "        \n",
    "        text = row['clean_tweet']\n",
    "\n",
    "        if type(text) is not str:\n",
    "            text = default_word\n",
    "\n",
    "        df_vec_partial.loc[i] = get_vec(text, _min).tolist() \\\n",
    "            + get_vec(text, _max).tolist() + get_vec(text, _avg).tolist() \\\n",
    "            + h[x+i,:].tolist()\n",
    "\n",
    "        i+=1\n",
    "        \n",
    "    df_vec_partial['target'] = df[x*c:(x+1)*c]['target'].to_list()\n",
    "    df_vec_partial.to_csv(f'./data/tweets_partials/tweet_prepared_{x}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
